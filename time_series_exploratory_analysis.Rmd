---
title: "Watts Up CA"
author: "Claire Boyd, Kathryn Link-Oberstar, Megan Moore, Eshan Prasher"
date: "`r Sys.Date()`"
output: pdf_document
---


```{r}
suppressMessages({
  library(readxl)
  library(dplyr)
  library(forecast)
  library(tidyverse)
  library(tsibble)
  library(fpp)
  library(tseries)
  library(tidyr)
})

# change this depending on where you want to start your wd()
# setwd("~/Documents/time_series/final_project/watts-up-ca/")
```

## LOAD DATA

```{r}
# Load the Energy Consumption Data
pathname <- "./data/electricity_consumption_data/"
files <- list.files(path=pathname, pattern = "\\.(xls|xlsx)$")
print(files)
combined_data <- data.frame()

# Convert Files to df, collapse the 3 header columns into 1 header
for (file in files) {
  full_path <- file.path(pathname, file)
  data <- read_excel(full_path, col_names = FALSE)
  new_headers <- apply(data[1:3, ], 2, function(x) paste(na.omit(x), collapse = " "))
  names(data) <- new_headers
  data <- data[-(1:3), ]
  data <- data.frame(lapply(data, function(x) {
    if(is.factor(x)) as.character(x) else x
  }), stringsAsFactors = FALSE)
  combined_data <- bind_rows(combined_data, data)
}

# Make Month and Year Numeric
combined_data$Month <- as.numeric(combined_data$Month)
combined_data$Year <- as.numeric(combined_data$Year)

# Filter data to exclude NA and only include CA
combined_data <- combined_data %>% filter(!is.na(Year) & !is.na(Month) & combined_data$State == 'CA')

# Sort by year and month
sorted_data <- combined_data %>%
  arrange(Year, Month)
```

## RESIDENTIAL ENERGY CONSUMPTION

```{r}
residential <- ts(as.numeric(sorted_data$RESIDENTIAL.Sales.Megawatthours), start = c(1990, 1), frequency = 12)
plot(residential)
```

### Evaluate Dataset

Observations from the residential ts() object before any transformations:

-   *trend*: positive, the residential consumption of electricity is increasing with time.

-   *seasonality*: strong seasonality at the annual level. It looks like there are two peaks of electricity use each year (which makes sense if we think about A/C and heat uses as both tied to electricity).

-   *type of seasonality*: the variance of the seasonality fluctuations change (increase) as a function of time, so this looks to be a multiplicative seasonality component.

-   *stationarity*: given the above observations that we can observe both trends and seasonal effects, we can guess that the time series data is not yet stationary. We can test this below.

### Compare to other types of electricity consumption

**COMMERCIAL ENERGY CONSUMPTION**

```{r}
commercial <- ts(sorted_data$COMMERCIAL.Sales.Megawatthours, start = c(1990, 1), frequency = 12)
plot(commercial)
```

**TRANSPORTATION ENERGY CONSUMPTION**

```{r}
transportation <- ts(sorted_data$TRANSPORTATION.Sales.Megawatthours[!is.na(as.numeric(sorted_data$TRANSPORTATION.Sales.Megawatthours))], start = c(1990, 1), frequency = 12)
plot(transportation) # not sure if this is totally valid removing the NA's

transportation <- ts(sorted_data$TRANSPORTATION.Sales.Megawatthours, start = c(1990, 1), frequency = 12)
plot(transportation)
```

**TOTAL ENERGY CONSUMPTION**

```{r}
total <- ts(sorted_data$TOTAL.Sales.Megawatthours, start = c(1990, 1), frequency = 12)
plot(total)
```

General takeaways:

-   trends/seasonality does not look the same across consumption type (residential, commercial, transportation, etc).
-   transportation has a bunch of missingness (especially in the earlier years)
    -   it also took a much more noticeable COVID hit than the other sectors (and surprisingly consumption didn't go up that much in 2020 for residential which is kind of surprising given that people were theoretically at home more)
-   commercial sees a much steeper upward trend in the early 2000's

## SUPPLEMENTARY DATA

### Monthly Temperature Data for all of California

```{r, results='hold'}
# Load the Monthly Temperature Data

temp_data <- read_excel('./data/ca_monthly_temp_data.xlsx')
temp_ts <- ts(temp_data$Value, frequency = 12, start=c(1990,1))
plot(temp_ts, ylab="Temperature", main="Monthly Average Temperature 1990-2023")
```

```{r, results=TRUE}
plot(decompose(temp_ts))
```

With the temperature data we do not seem to have a notable upward or downward trend, though from the decomposition it might be experiencing a slight positive trend. We see a consistent 12-month seasonal trend that seems to be additive in nature given that the amplitude of the variance is not growing from the looks of it.

### Yearly Population Data

```{r}

pop_data <- read_excel('./data/ca_pop_1990_2022.xlsx', sheet="Data")
pop_ts <- ts(pop_data[2], frequency = 1, start=1990)
plot(pop_ts, ylab="Population (millions)", main="Population of California 1990-2022")
```

For the population data we see an upward trend that may be leveling off (would maybe need damping) , or this may be due to the effects of covid and will continue linearly increasing. There is no seasonality as these are annual population estimates.

### UNEMPLOYMENT DATA

```{r}
unemp_data <- read_excel('./data/bls_cali_seasonal_adj_1990_2023.xlsx', 
                         sheet="Sheet1")

unemp_rate_ts <- ts(unemp_data$`unemployment rate`, frequency = 12, start =c(1990,1))
plot(unemp_rate_ts, ylab="Unemployment rate", main="Monthly unemployement rate in
     California 1990-2023")
```

\*\*\* General observations: - Almost at its lowest levels pre-Covid and post 2022 - Before Covid, peak was in 2009 - after the financial crisis of 2008. However, then, double-digit unemployed stayed till 2012 which hasn't happened after Covid. - Before 2008 recession, near double digit unemployement in 1992-93 due to national recession - job cuts in manufacturing

```{r}
plot(decompose(unemp_rate_ts))
```

## TRAIN TEST SPLITS

```{r}
#train/test split at 1990
train_res_1990 <- window(residential, start=1990, end=c(2022, 6))
test_res_1990 <- window(residential, start=c(2022, 7), end=c(2023, 7))
train_test_res_1990 <- window(residential, start=1990, end=c(2023, 7))

#train/test split at 2005
train_res_2005 <- window(residential, start=2005, end=c(2022, 6))
test_res_2005 <- window(residential, start=c(2022, 7), end=c(2023, 7))
train_test_res_2005 <- window(residential, start=1990, end=c(2023, 7))


#train/test split at 2013
train_res_2013 <- window(residential, start=2013, end=c(2022, 6))
test_res_2013 <- window(residential, start=c(2022, 7), end=c(2023, 7))
train_test_res_2013 <- window(residential, start=2013, end=c(2023, 7))
```

## BASELINE MODELS

### Seasonal Naive baseline model

The point forecasts are the same for each of these models, but the information criteria estimates are different (and the confidence interval bands are different for each)

MAPE for 1990: 5.521388 RMSE for 1990: 562980.8

MAPE for 2005: 6.094906 RMSE for 2005: 657427

MAPE for 2013: 7.692256 RMSE for 2013: 797692.8

Both are increasing as we narrow the window size.

```{r}
snaive_model_1990 = snaive(train_res_1990, h=12)
accuracy(snaive_model_1990)

snaive_model_2005 = snaive(train_res_2005, h=12)
accuracy(snaive_model_2005)

snaive_model_2013 = snaive(train_res_2013, h=12)
accuracy(snaive_model_2013)

plot(snaive_model_1990)
```

### Exponential Smoothing

model type: ETS(M,N,M) MAPE for 1990: 4.667398 RMSE for 1990: 466188.8

model type: ETS(M,N,A) MAPE for 2005: 5.300409 RMSE for 2005: 535969.3

model type: ETS(M,N,M) MAPE for 2013: 5.55343 RMSE for 2013: 573835.4

Weird that with different windows a different ETS model is being specified. Similar to seasonal naive, as we increase the size of training data, we increase the MAPE and RSME values.

```{r}
ets_model_1990 = ets(train_res_1990)
summary(ets_model_1990)

ets_model_2005 = ets(train_res_2005)
summary(ets_model_2005)

ets_model_2013 = ets(train_res_2013)
summary(ets_model_2013)
```

```{r}
plot(forecast(ets_model_1990, h=12))
```

### Linear Regression

MAPE for 1990: 6.877424 RMSE for 1990: 610919.6

MAPE for 2005: 5.426142 RMSE for 2005: 533482.7

MAPE for 2013: 5.572336 RMSE for 2013: 570245.5

Unlike the other baselines, this slightly improves with time and then gets worse again.

```{r}
tslm_model_1990 = tslm(train_res_1990 ~ trend + season)
summary(tslm_model_1990)
accuracy(tslm_model_1990)

tslm_model_2005 = tslm(train_res_2005 ~ trend + season)
summary(tslm_model_2005)
accuracy(tslm_model_2005)

tslm_model_2013 = tslm(train_res_2013 ~ trend + season)
summary(tslm_model_2013)
accuracy(tslm_model_2013)
```

```{r}
plot(forecast(tslm_model_1990, h=12))
```

```{r}
tslm_model_1990
```

### Linear Regression with temperature and unemployment

## First, creating a dataframe with all variables from 1990-2022

```{r}
# Create a sequence of years and months from 1990 to 2022
dates <- seq(as.Date("1990-01-01"), as.Date("2022-12-01"), by="months")

# Create a dataframe with Year and Month columns
residential_reg_df <- data.frame(Year = as.integer(format(dates, "%Y")), 
                                 Month = as.integer(format(dates, "%m")))

# Extract yearly population values from the pop_data dataframe
residential_reg_df$pop <- as.numeric(rep(pop_data$population, each = 12))

# Extract consumption data from sorted_data
residential_reg_df$elec_cons <- as.numeric(subset(sorted_data, Year >= 1990 & 
                                        Year <= 2022)$RESIDENTIAL.Sales.Megawatthours)

# Extract temperature data from temp_data
# First, we create a new 'Year' column by extracting the first four characters from 'Date'

temp_data$Year <- as.numeric(substr(temp_data$Date, 1, 4))
residential_reg_df$temp <- as.numeric(subset(temp_data,Year >= 1990 & 
                                        Year <= 2022)$Value)

# Extract unemployment rate data from unemp_data
residential_reg_df$unemp_rate <- as.numeric(subset(unemp_data, Year >= 1990 & 
                                              Year <= 2022)$`unemployment rate`)

# Calculate consumption per capita by dividing consumption by population
# consumption is MWhrs per million population
residential_reg_df$cons_capita <- residential_reg_df$elec_cons/(residential_reg_df$pop)

# Optionally, you can set row names if needed
rownames(residential_reg_df) <- NULL

# Print the first few rows to verify
head(residential_reg_df, 15)

```

```{r}
# Adding normalized values of 'temperature' and 'unemployment rate' to the dataframe 
residential_reg_df$norm_temp <- scale(residential_reg_df$temp)
residential_reg_df$norm_unemp_rate <- scale(residential_reg_df$unemp_rate)

# First, we will regress only with temperature
y_var <- ts(residential_reg_df$cons_capita, start = c(1990,1), frequency = 12)
x1_var <- ts(residential_reg_df$temp, start = c(1990,1), frequency = 12)
  
tslm_residential_temp <- tslm(y_var ~ x1_var, lambda = 'auto')  

# Summarize the regression results
summary(tslm_residential_temp)
accuracy(tslm_residential_temp)


# Now we add unemployment to this
x2_var <- ts(residential_reg_df$unemp_rate, start = c(1990,1), frequency = 12)
tslm_residential_temp_unemp <- tslm(y_var ~ x1_var + x2_var, lambda = 'auto') 

# Summarize the regression results
summary(tslm_residential_temp_unemp)
accuracy(tslm_residential_temp_unemp)

# Then just regress with population 
x3_var <- ts(residential_reg_df$unemp_rate, start = c(1990,1), frequency = 12)
tslm_residential_pop <- tslm(y_var ~ x3_var, lambda = 'auto')  

# Summarize the regression results
summary(tslm_residential_pop)
accuracy(tslm_residential_pop)
```

\*\*\* Key takeaways: - temperature is statistically significant, unemployment rate is not - maybe look at other labour market variables (or normalize and check), population is also not statistically significant indicating that consumption increases are not necessarily attributable to more people using electricity.

-   Do we look at adjusted R\^2? why not?

-   forecast is not working on this for some reason - need to debug - I think this is due to the fact that we need to know the "future" independent variables

```{r}
# plot(forecast(tslm_residential_pop), h=12)
# TODO forecast 
# 1) train model on only training set 
# 2) set temp/pop/whatever is being regressed on aside for test set
# 3) figure out how to provide the x_vals for the forcast
```

```{r}
### Experimenting with other regressions

y_var <- ts(residential_reg_df$cons, start = c(1990,1), frequency = 12) # consumption instead of consumption per capita

tslm_residential_tot_temp <- tslm(y_var ~ x1_var, lambda = 'auto') 

accuracy(tslm_residential_tot_temp)
summary(tslm_residential_tot_temp)

# Takeaway: No impact of standardizing consumption by population in previous regression
```

\*\*\* Next step: combine Claire's autoregression with temperature (and any other variable) to create models discussed in Lecture-7

### Arima

*Evaluate Distribution*

```{r}
shapiro.test(train_res_1990)
```

-   p-value below significance threshold
-   data not normally distributed and will benefit from Box-Cox Transformation

```{r}
lambda <- BoxCox.lambda(train_res_1990)
train_res_1990_transformed <- BoxCox(train_res_1990, lambda)
plot(train_res_1990)
print(lambda)
```

-   The Box-Cox $\lambda = -0.9999$ which is extremely close to -1 indicating that an inverse transformation plus 1 should be applied to smooth the variance.

*Evaluate Stationarity*

```{r}
kpss_result <- kpss.test(train_res_1990_transformed)
print(kpss_result)
```

-   p-value below 5%
-   Reject the null hypothesis that series is stationary
-   Non stationary series

*Make Stationary with differencing*

```{r}
train_res_1990_transformed_first_diff <- diff(train_res_1990_transformed, differences = 1)
plot(train_res_1990_transformed_first_diff, main="1st Order Differenced Data")
kpss_result <- kpss.test(train_res_1990_transformed_first_diff)
print(kpss_result)
```

-   After 1st order differencing, p-value is 0.1
-   After 1 round of differencing, data is stationary in the mean

*Seasonal Differencing*

```{r}
train_res_1990_transformed_first_diff_sdiff <- diff(train_res_1990_transformed_first_diff, lag = 12, differences = 1)
plot(train_res_1990_transformed_first_diff_sdiff)
```

*ACF & PACF*

```{r}
Acf(train_res_1990_transformed_first_diff_sdiff)
Pacf(train_res_1990_transformed_first_diff_sdiff)
```

-   Seasonality

*Arima Model*

```{r}
plot(train_res_1990_transformed_first_diff_sdiff)
```

*Auto Arima*

```{r}
residential_auto_arima_1990 <- auto.arima(train_res_1990, 
                                     seasonal = TRUE, 
                                     trace = FALSE, 
                                     stepwise = FALSE, 
                                     #approximation = FALSE, 
                                     allowdrift = TRUE, 
                                     lambda = 'auto')
summary(residential_auto_arima_1990)
```

```{r}
checkresiduals(residential_auto_arima_1990)
```


-   Residuals are not white noise

```{r}
residential_auto_arima_2005 <- auto.arima(train_res_2005, 
                                     seasonal = TRUE, 
                                     trace = FALSE, 
                                     stepwise = FALSE, 
                                     #approximation = FALSE, 
                                     allowdrift = TRUE, 
                                     lambda = 'auto')
summary(residential_auto_arima_2005)
```

```{r}
checkresiduals(residential_auto_arima_2005)
```


```{r}
residential_auto_arima_2013 <- auto.arima(train_res_2013, 
                                     seasonal = TRUE, 
                                     trace = FALSE, 
                                     stepwise = FALSE, 
                                     #approximation = FALSE, 
                                     allowdrift = TRUE, 
                                     lambda = 'auto')
summary(residential_auto_arima_2013)
```

```{r}
checkresiduals(residential_auto_arima_2013)
```
## MODEL TESTING

*Models to Test*

```{r}
models_to_test <- list(
  list("Residential Auto Arima - 1990", residential_auto_arima_1990, test_res_1990, 12),
  list("Residential Auto Arima - 2005", residential_auto_arima_2005, test_res_2005, 12),
  list("Residential Auto Arima - 2013", residential_auto_arima_2013, test_res_2013, 12),
  list("Residential Seasonal Naive - 1990", snaive_model_1990, test_res_1990, 12),
  list("Residential Seasonal Naive - 2005", snaive_model_2005, test_res_2005, 12),
  list("Residential Seasonal Naive - 2013", snaive_model_2013, test_res_2013, 12),
  list("ETS - 1990", ets_model_1990, test_res_1990, 12),
  list("ETS - 2005", ets_model_2005, test_res_2005, 12),
  list("ETS - 2013", ets_model_2013, test_res_2013, 12),
  list("Linear Regression - 1990", tslm_model_1990, test_res_1990, 12),
  list("Linear Regression - 2005", tslm_model_2005, test_res_2005, 12),
  list("Linear Regression - 2013", tslm_model_2013, test_res_2013, 12)

)

evaluate_models <- function(models_to_test) {
  results <- list()
  
  for (model_info in models_to_test) {
    
    model_name <- model_info[[1]]
    model <- model_info[[2]]
    test_data <- model_info[[3]]
    horizon <- model_info[[4]]
    
    # Forecasting
    model_forecast <- forecast(model, h=horizon)
    
    # Error calculation
    errors <- accuracy(model_forecast, test_data)
    MAE <- errors["Test set", "MAE"]
    RMSE <- errors["Test set", "RMSE"]
    AICc <- ifelse(exists("aicc", where = model), model$aicc, NA)
    
    results[[model_name]] <- list(MAE = MAE, RMSE = RMSE, AICc = AICc, Forecast = model_forecast)
    
    cat(paste0(model_name, " Model Performance on Test Data: \n",
               "MAE: ", MAE, "\n",
               "RMSE: ", RMSE, "\n",
               "AICc: ", AICc, "\n\n"))
  }
  
  return(results)
}

results <- evaluate_models(models_to_test)


```
